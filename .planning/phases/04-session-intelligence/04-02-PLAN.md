---
phase: 04-session-intelligence
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - lib/session-evaluator.js
  - lib/state.js
autonomous: true

must_haves:
  truths:
    - "SessionEvaluator.evaluate() captures tmux output, gets git progress, calls LLM-as-judge via claudePWithSemaphore, and returns a structured evaluation"
    - "Evaluation result includes score (1-5), recommendation (continue/retry/escalate/complete), accomplishments[], failures[], and reasoning"
    - "Evaluation is written to .orchestrator/evaluation.json per project AND logged to state evaluationHistory[]"
    - "state.logEvaluation() appends to evaluationHistory[] capped at 100 entries"
    - "state.getRecentEvaluations() returns the last N evaluations"
  artifacts:
    - path: "lib/session-evaluator.js"
      provides: "LLM-as-judge session quality scoring"
      exports: ["SessionEvaluator", "EVALUATION_SCHEMA"]
    - path: "lib/state.js"
      provides: "Evaluation history persistence methods"
      contains: "logEvaluation"
  key_links:
    - from: "lib/session-evaluator.js"
      to: "lib/exec.js"
      via: "claudePWithSemaphore for LLM judge call"
      pattern: "claudePWithSemaphore"
    - from: "lib/session-evaluator.js"
      to: "lib/git-tracker.js"
      via: "getProgress() for git stats"
      pattern: "gitTracker\\.getProgress"
    - from: "lib/session-evaluator.js"
      to: "tmux CLI"
      via: "execSync tmux capture-pane for session output"
      pattern: "tmux capture-pane"
---

<objective>
Create the session evaluation module (SESS-02) that combines objective git signals with LLM-as-judge assessment to score completed sessions, plus add evaluation history methods to state.js.

Purpose: This is the core intelligence loop -- the orchestrator learns whether sessions actually accomplish anything, enabling smarter decisions about retries, escalation, and session prompts.
Output: New lib/session-evaluator.js with evaluate() method, and two new methods on StateManager for evaluation persistence.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-session-intelligence/04-RESEARCH.md
@lib/exec.js
@lib/state.js
@lib/git-tracker.js
@lib/session-manager.js
@test/helpers.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create lib/session-evaluator.js</name>
  <files>lib/session-evaluator.js</files>
  <action>
Create a SessionEvaluator class following the existing DI pattern.

**Constructor:**
```javascript
constructor({ gitTracker, state, config }) {
  this.gitTracker = gitTracker;
  this.state = state;
  this.config = config;
}
```

**EVALUATION_SCHEMA** (module-level constant):
Define a JSON schema string for the evaluation response:
```javascript
const EVALUATION_SCHEMA = JSON.stringify({
  type: 'object',
  properties: {
    score: { type: 'integer', minimum: 1, maximum: 5 },
    recommendation: { type: 'string', enum: ['continue', 'retry', 'escalate', 'complete'] },
    accomplishments: { type: 'array', items: { type: 'string' } },
    failures: { type: 'array', items: { type: 'string' } },
    reasoning: { type: 'string' },
  },
  required: ['score', 'recommendation', 'accomplishments', 'failures', 'reasoning'],
});
```

**async evaluate({ projectName, projectDir, sessionName, startedAt, headBefore, prompt })**

1. **Capture tmux output** (BEFORE the session is destroyed):
   - `tmux capture-pane -t "${sessionName}" -p -S -200 -J` via execSync with 5s timeout
   - Strip ANSI escape codes: `output.replace(/\x1B\[[0-9;]*[a-zA-Z]/g, '')`
   - Trim to last 2000 chars for the LLM prompt
   - Wrap in try/catch -- if session is already gone, use empty string

2. **Get git progress**:
   - `this.gitTracker.getProgress(projectDir, startedAt)` -- uses the session start time as the "since" window
   - If headBefore was null (no commits at session start), still works -- git --since handles it

3. **Calculate duration**:
   - `const durationMin = Math.round((Date.now() - new Date(startedAt).getTime()) / 60000)`

4. **Build evaluation prompt** (keep under 3000 chars):
   ```
   You are evaluating the quality of an automated coding session.
   Analyze the objective evidence and terminal output, then score the session.

   RUBRIC:
   Score 1 (Failed): No commits, no meaningful changes, or only errors in output.
   Score 2 (Minimal): Some activity but key objectives not addressed. Mostly setup/config.
   Score 3 (Acceptable): Partial progress. Some real code changes toward objectives.
   Score 4 (Good): Clear progress on objectives. Multiple meaningful commits. Tests pass.
   Score 5 (Excellent): All stated objectives completed. Clean commits, tests pass, docs updated.

   OBJECTIVE EVIDENCE:
   - Commits since session start: {commitCount}
   - Files changed: {filesChanged}
   - Lines added: {insertions}, Lines removed: {deletions}
   - Last commit: "{lastCommitMessage}"
   - Session duration: {durationMin} minutes
   {If noGit: "- No git repository found. Evaluate based on terminal output only."}

   SESSION PROMPT (what was asked):
   {prompt, truncated to 500 chars}

   TERMINAL OUTPUT (last lines):
   {tmuxOutput, truncated to 2000 chars}

   Think step by step: What did this session accomplish relative to its objectives?
   Then provide your evaluation.
   ```

5. **Call LLM-as-judge**:
   - `const { claudePWithSemaphore } = require('./exec')`
   - `const result = await claudePWithSemaphore(evalPrompt, { jsonSchema: EVALUATION_SCHEMA, timeout: 30000 })`
   - Parse result with JSON.parse (defense-in-depth, same pattern as ai-brain.js)
   - Wrap in try/catch -- on failure, return a fallback evaluation with score based on commit count: 0 commits = 1, 1-2 = 3, 3+ = 4

6. **Build evaluation record**:
   ```javascript
   const evaluation = {
     sessionId: sessionName,
     projectName,
     startedAt,
     stoppedAt: new Date().toISOString(),
     durationMinutes: durationMin,
     gitProgress: {
       commitCount: gitProgress.commitCount,
       insertions: gitProgress.insertions,
       deletions: gitProgress.deletions,
       filesChanged: gitProgress.filesChanged,
       lastCommitMessage: gitProgress.lastCommitMessage,
       noGit: gitProgress.noGit || false,
     },
     score: parsed.score,
     recommendation: parsed.recommendation,
     accomplishments: parsed.accomplishments,
     failures: parsed.failures,
     reasoning: parsed.reasoning,
     evaluatedAt: new Date().toISOString(),
   };
   ```

7. **Persist**:
   - Write to `path.join(projectDir, '.orchestrator', 'evaluation.json')` -- create .orchestrator dir if needed
   - Log to state: `this.state.logEvaluation(this.state.load(), evaluation)`
   - Return the evaluation object

**Export both:** `module.exports = { SessionEvaluator, EVALUATION_SCHEMA };`

Do NOT:
- Evaluate synchronously during session stop (this is async, called after stop)
- Send more than 2000 chars of tmux output to the LLM (truncate)
- Use the `-e` flag on tmux capture-pane (preserves ANSI escapes -- we want plain text)
  </action>
  <verify>
`node -e "const { SessionEvaluator, EVALUATION_SCHEMA } = require('./lib/session-evaluator'); console.log('Schema:', EVALUATION_SCHEMA.substring(0, 80)); console.log('Class:', typeof SessionEvaluator)"` -- should show schema and 'function'.

`node -e "JSON.parse(require('./lib/session-evaluator').EVALUATION_SCHEMA)"` -- schema parses as valid JSON.
  </verify>
  <done>
SessionEvaluator class loads, EVALUATION_SCHEMA is valid JSON, evaluate() method accepts session metadata and returns structured evaluation via LLM-as-judge.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add evaluation history methods to state.js</name>
  <files>lib/state.js</files>
  <action>
Add two new methods to the StateManager class, following the exact same pattern as logDecision/getRecentDecisions:

**logEvaluation(state, evaluation)**
- Initialize `state.evaluationHistory` as `[]` if missing
- Push the evaluation object
- Cap at 100 entries: `if (state.evaluationHistory.length > 100) { state.evaluationHistory = state.evaluationHistory.slice(-100); }`
- Call `this.save(state)`

**getRecentEvaluations(state, count = 5)**
- Return `(state.evaluationHistory || []).slice(-count)`

Also update the `load()` method's default return object to include `evaluationHistory: []`.

Place these methods after the existing `logExecution` / error retry methods section. Add a comment header: `// --- Phase 4: Session Evaluation History ---`

Do NOT:
- Change any existing method signatures
- Modify the stateFile path or save logic
- Add any new constructor parameters
  </action>
  <verify>
`node -e "const SM = require('./lib/state'); const s = new SM('/tmp/test-state.json'); const st = s.load(); console.log('evaluationHistory' in st); s.logEvaluation(st, { score: 4, projectName: 'test' }); console.log(s.getRecentEvaluations(st)); require('fs').unlinkSync('/tmp/test-state.json')"` -- should show `true` and an array with the logged evaluation.
  </verify>
  <done>
StateManager has logEvaluation() and getRecentEvaluations() methods that persist evaluation history capped at 100 entries.
  </done>
</task>

</tasks>

<verification>
- `node -e "require('./lib/session-evaluator')"` loads without error
- `node -e "require('./lib/state')"` loads without error
- EVALUATION_SCHEMA parses as valid JSON with score, recommendation, accomplishments, failures, reasoning fields
- state.logEvaluation() persists to disk and getRecentEvaluations() retrieves entries
- No new npm dependencies added
</verification>

<success_criteria>
SessionEvaluator is ready to be called after session stop with session metadata and returns a structured evaluation. State persists evaluation history. EVALUATION_SCHEMA is compatible with claudePWithSemaphore's --json-schema flag.
</success_criteria>

<output>
After completion, create `.planning/phases/04-session-intelligence/04-02-SUMMARY.md`
</output>
